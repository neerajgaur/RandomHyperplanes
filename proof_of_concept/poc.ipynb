{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Proof of concept for the random-cut-hyperplanes idea \"\"\"\n",
    "import sys\n",
    "import numpy as np\n",
    "from scipy.stats import scoreatpercentile\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# sys.setrecursionlimit(50000)\n",
    "\n",
    "# Steps for the algorithm\n",
    "#\n",
    "# Given a dataset, X:\n",
    "#   1. Get the feature ranges for each feature within X.\n",
    "#   2. Sample a random point between each feature.\n",
    "#   3. Create a hyperplane using each of these points.\n",
    "#   4. Get the normal to that plane.\n",
    "#   5. Determine which side of the plane the point lies on.\n",
    "#   6. Repeat for the two sides.\n",
    "#   7. If an individual point is left, stop iteration\n",
    "#\n",
    "class IsolationForest(object):\n",
    "    def __init__(self, n_estimators=10):\n",
    "        self.n_estimators = n_estimators\n",
    "\n",
    "    def fit(self, points):\n",
    "        self.trees = []\n",
    "        for i in range(self.n_estimators):\n",
    "            self.trees.append(IsolationTree(points))\n",
    "        return self\n",
    "\n",
    "    def decision_function(self, points):\n",
    "        depths = np.array([tree.decision_function(points) for tree in self.trees])\n",
    "        mean_depths = np.mean(depths, axis=0)\n",
    "        # Normalize the points\n",
    "        scores = 2**-(mean_depths / average_path_length(points.shape[0]))\n",
    "        return scores\n",
    "\n",
    "    def get_depths(self, points):\n",
    "        depths = np.array([tree.decision_function(points) for tree in self.trees])\n",
    "        mean_depths = np.mean(depths, axis=0)\n",
    "        return mean_depths\n",
    "\n",
    "\n",
    "class IsolationTree(object):\n",
    "    def __init__(self, points, group_threshold=15, depth=1):\n",
    "        self.child_left = self.child_right = None\n",
    "        self.points = points\n",
    "        self.group_threshold = group_threshold\n",
    "        self.num_points = self.points.shape[0]\n",
    "        self.split(self.points, depth)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"<{self.__class__.__name__} num_points:{self.num_points}>\"\n",
    "\n",
    "    @property\n",
    "    def is_leaf(self):\n",
    "        return self.child_left == None and self.child_right == None\n",
    "\n",
    "    def split(self, points, depth=1):\n",
    "        node, points_left, points_right = self.get_split(points)\n",
    "\n",
    "        if not node or depth >= 50:\n",
    "            return self\n",
    "\n",
    "        self.node = node\n",
    "        self.child_left = IsolationTree(points_left, depth=depth + 1)\n",
    "        self.child_right = IsolationTree(points_right, depth=depth + 1)\n",
    "\n",
    "    def decision_function(self, points):\n",
    "        return np.array([self.get_depth(point) for point in points])\n",
    "\n",
    "    def get_depth(self, point):\n",
    "        if self.is_leaf:\n",
    "            return 1\n",
    "        else:\n",
    "            if self.node.is_point_left(point):\n",
    "                return 1 + self.child_left.get_depth(point)\n",
    "            else:\n",
    "                return 1 + self.child_right.get_depth(point)\n",
    "\n",
    "    def get_split(self, points):\n",
    "        if self.num_points <2:\n",
    "            return (None, None, None)\n",
    "        split_feature = sample_feature(points)\n",
    "        split_threshold = sample_split_threshold(points, split_feature)\n",
    "        node = Node(split_threshold, split_feature)\n",
    "\n",
    "        positions = node.position_of_points(points)\n",
    "\n",
    "        points_left = points[positions]\n",
    "        points_right = points[np.logical_not(positions)]\n",
    "\n",
    "        return (node, points_left, points_right)\n",
    "\n",
    "\n",
    "class Node(object):\n",
    "    def __init__(self, threshold, feature):\n",
    "        self.threshold = threshold\n",
    "        self.feature = feature\n",
    "\n",
    "    def is_point_left(self, point):\n",
    "        return point[self.feature] < self.threshold\n",
    "\n",
    "    def position_of_points(self, points):\n",
    "        return np.array([self.is_point_left(point) for point in points])\n",
    "    \n",
    "def get_feature_ranges(points):\n",
    "    \"\"\" Yields the min and max of each feature in a vector. \"\"\"\n",
    "    points_transpose = np.transpose(points)\n",
    "    for point in points_transpose:\n",
    "        yield (np.min(point), np.max(point))\n",
    "        \n",
    "def sample_feature(point):\n",
    "    length = point.shape[-1]\n",
    "    return np.random.randint(low=0, high=length)\n",
    "\n",
    "def sample_split_threshold(points, feature):\n",
    "    return list(generate_point(points))[feature]\n",
    "\n",
    "def average_path_length(n):\n",
    "    return 2 * harmonic_approx(n - 1) - (2 * (n - 1) / n)\n",
    "\n",
    "def harmonic_approx(n):\n",
    "    return np.log(n) + 0.5772156649\n",
    "\n",
    "class RandomHyperplanes(object):\n",
    "    def __init__(self, n_estimators=10):\n",
    "        self.n_estimators = n_estimators\n",
    "\n",
    "    def fit(self, points):\n",
    "        self.planes = []\n",
    "        for i in range(self.n_estimators): \n",
    "            self.planes.append(HyperplaneCollection(points))\n",
    "        return self\n",
    "\n",
    "    def decision_function(self, points):\n",
    "        depths = np.array([plane.decision_function(points) for plane in self.planes])\n",
    "        mean_depths = np.mean(depths, axis=0)\n",
    "        # Normalize the points\n",
    "        scores = 2**-(mean_depths / average_path_length(points.shape[0]))\n",
    "        return scores\n",
    "\n",
    "    def get_depths(self, points):\n",
    "        depths = np.array([plane.decision_function(points) for plane in self.planes])\n",
    "        mean_depths = np.mean(depths, axis=0)\n",
    "        return mean_depths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def _gen_hard_data(n, p, infection_pct, variance=10.0, mu=5.0):\n",
    "    X = np.random.randn(n, p)\n",
    "\n",
    "    # hard data\n",
    "    # Weight it to the number of features\n",
    "    is_anomaly = np.random.rand(n, p) < (infection_pct / p)\n",
    "    X[is_anomaly] = variance * np.random.randn() + mu\n",
    "\n",
    "    y = np.zeros(shape=(n,))\n",
    "\n",
    "    tmp = np.array([np.any(r) for r in is_anomaly])\n",
    "    y[tmp] = 1.0\n",
    "\n",
    "    return (X, y)\n",
    "\n",
    "def _gen_easy_data(n, p, infection_pct, variance=10.0, mu=5.0):\n",
    "    X = np.random.randn(n, p)\n",
    "    is_anomaly = np.random.choice(n, size=int(infection_pct*n), replace=False)\n",
    "    X[is_anomaly] = variance * np.random.randn(is_anomaly.shape[0], p) + mu\n",
    "    y = np.zeros(shape=(n,))\n",
    "    y[is_anomaly] = 1.0\n",
    "    return (X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def run_plane_simul(points, y):\n",
    "    print(\"Beginning plane fit...\")\n",
    "    rhp = RandomHyperplanes(n_estimators=N_ESTIMATORS)\n",
    "    rhp = rhp.fit(points)\n",
    "    print(\"done fitting\")\n",
    "\n",
    "#     scores = rhp.decision_function(points)\n",
    "#     threshold = scoreatpercentile(scores, 100 - SCORE_AT)\n",
    "#     anomalies = scores >= threshold\n",
    "#     y_pred = np.zeros(shape=anomalies.shape)\n",
    "#     y_pred[anomalies] = 1\n",
    "    \n",
    "#     \"\"\"\n",
    "#     correct_guesses = np.count_nonzero(y[np.where(scores <= threshold)])\n",
    "#     incorrect_guesses = y[np.where(scores <= threshold)].shape[0] - \\\n",
    "#         correct_guesses\n",
    "\n",
    "#     print(\"Correct guesses:\", correct_guesses)\n",
    "#     print(\"Incorrect guesses:\", incorrect_guesses)\n",
    "#     print(\"Expected\", np.count_nonzero(y), \"anomalies\")\n",
    "#     \"\"\"\n",
    "#     cnf_matrix = confusion_matrix(y, y_pred)\n",
    "\n",
    "#     \"\"\"\n",
    "#     tn, fp, fn, tp = cnf_matrix.ravel()\n",
    "#     print(f\"tp: {tp} \\ntn: {tn} \\nfp: {fp} \\nfn: {fn}\")\n",
    "#     \"\"\"\n",
    "#     cnf_matrix = cnf_matrix.astype('float') / \\\n",
    "#         cnf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "#     \"\"\"\n",
    "#     tn, fp, fn, tp = cnf_matrix.ravel()\n",
    "#     print(f\"Normalized \\ntp: {tp} \\ntn: {tn} \\nfp: {fp} \\nfn: {fn}\")\n",
    "#     \"\"\"\n",
    "#     print(cnf_matrix)\n",
    "\n",
    "    depths = rhp.get_depths(points)\n",
    "    anomalous_depths = depths[np.where(y==1.0)]\n",
    "    non_anomalous_depths = depths[np.where(y==0.0)]    \n",
    "    print(\"Average anomalous depth:\", np.mean(anomalous_depths))\n",
    "    print(\"Average non-anomalous depth:\", np.mean(non_anomalous_depths))\n",
    "    return (None, depths, None, y)\n",
    "\n",
    "\n",
    "def run_iforest_simul(points, y):\n",
    "    print(\"Beginning iforest fit...\")\n",
    "    iforest = IsolationForest(n_estimators=N_ESTIMATORS)\n",
    "    iforest = iforest.fit(points)\n",
    "    print(\"done fitting\")\n",
    "\n",
    "#     scores = iforest.decision_function(points)\n",
    "#     threshold = scoreatpercentile(scores, 100 - SCORE_AT)\n",
    "#     anomalies = scores >= threshold\n",
    "#     y_pred = np.zeros(shape=anomalies.shape)\n",
    "#     y_pred[anomalies] = 1\n",
    "\n",
    "#     \"\"\"\n",
    "#     correct_guesses = np.count_nonzero(y[np.where(scores <= threshold)])\n",
    "#     incorrect_guesses = y[np.where(scores <= threshold)].shape[0] - \\\n",
    "#         correct_guesses\n",
    "\n",
    "#     print(\"iforest Correct guesses:\", correct_guesses)\n",
    "#     print(\"iforest Incorrect guesses:\", incorrect_guesses)\n",
    "#     print(\"Expected\", np.count_nonzero(y), \"anomalies\")\n",
    "#     \"\"\"\n",
    "#     iforest_cnf_matrix = confusion_matrix(y, y_pred)\n",
    "#     \"\"\"\n",
    "#     tn, fp, fn, tp = iforest_cnf_matrix.ravel()\n",
    "#     print(f\"tp: {tp} \\ntn: {tn} \\nfp: {fp} \\nfn: {fn}\")\n",
    "#     \"\"\"\n",
    "#     iforest_cnf_matrix = iforest_cnf_matrix.astype('float') / \\\n",
    "#             iforest_cnf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "#     print(iforest_cnf_matrix)\n",
    "\n",
    "#     \"\"\"\n",
    "#     tn, fp, fn, tp = iforest_cnf_matrix.ravel()\n",
    "#     print(f\"Normalized \\ntp: {tp} \\ntn: {tn} \\nfp: {fp} \\nfn: {fn}\")\n",
    "#     \"\"\"\n",
    "    depths = iforest.get_depths(points)\n",
    "    anomalous_depths = depths[np.where(y==1.0)]\n",
    "    non_anomalous_depths = depths[np.where(y==0.0)]    \n",
    "    print(\"Average anomalous depth:\", np.mean(anomalous_depths))\n",
    "    print(\"Average non-anomalous depth:\", np.mean(non_anomalous_depths))\n",
    "    return (None, depths, None, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HyperplaneCollection(object):\n",
    "    def __init__(self, points, group_threshold=15, depth=1):\n",
    "        self.child_left = self.child_right = None\n",
    "        self.points = points\n",
    "        self.group_threshold = group_threshold\n",
    "        self.num_points = self.points.shape[0]\n",
    "        self.split(self.points, depth)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"<{self.__class__.__name__} num_points:{self.num_points}>\"\n",
    "\n",
    "    @property\n",
    "    def is_leaf(self):\n",
    "        return self.child_left == None and self.child_right == None\n",
    "\n",
    "    def split(self, points, depth=1):\n",
    "        plane, points_left, points_right = self.get_split(points)\n",
    "        if not plane or depth >= 50:\n",
    "            return self\n",
    "        self.splitting_plane = plane\n",
    "        self.child_left = HyperplaneCollection(points_left, depth=depth + 1)\n",
    "        self.child_right = HyperplaneCollection(points_right, depth=depth + 1)\n",
    "\n",
    "    def decision_function(self, points):\n",
    "        return np.array([self.get_depth(point) for point in points])\n",
    "\n",
    "    def get_depth(self, point):\n",
    "        if self.is_leaf:\n",
    "            return 1\n",
    "        else:\n",
    "            if self.splitting_plane.point_relative_to_plane(point) < 0:\n",
    "                return 1 + self.child_left.get_depth(point)\n",
    "            else:\n",
    "                return 1 + self.child_right.get_depth(point)\n",
    "\n",
    "    def get_split(self, points):\n",
    "        if self.num_points < 2:\n",
    "            return (None, None, None)\n",
    "        \n",
    "        splitting_plane = generate_splitting_plane(points)\n",
    "        positions = splitting_plane.position_of_points(points)\n",
    "        #split_point = np.random.uniform(low=np.min(positions), high=np.max(positions))\n",
    "        split_point = np.dot(splitting_plane.normal, splitting_plane.origin)\n",
    "        positions_x = np.array([splitting_plane.point_relative_to_plane_x(point) for point in points])\n",
    "        split_point_x = np.random.uniform(low=np.min(positions), high=np.max(positions))\n",
    "        #positions_x = positions_x - split_point\n",
    "        #print(positions-positions_x)\n",
    "        #positions -= split_point\n",
    "        points_left = points[np.where(positions < split_point)]\n",
    "        points_right = points[np.where(positions > split_point)]\n",
    "        return (splitting_plane, points_left, points_right)\n",
    "\n",
    "\n",
    "class Hyperplane(object):\n",
    "    def __init__(self, origin, normal):\n",
    "        self.origin = origin\n",
    "        self.normal = normal\n",
    "\n",
    "    def point_relative_to_plane(self, point):\n",
    "        result = np.dot(self.normal, point) - np.dot(self.normal, self.origin)\n",
    "        result = np.dot(self.normal, point)\n",
    "        return result\n",
    "\n",
    "    def point_relative_to_plane_x(self, point):\n",
    "        result = np.dot(self.normal, point)\n",
    "        return result\n",
    "\n",
    "    def position_of_points(self, points):\n",
    "        offset = np.dot(self.normal, self.origin)\n",
    "        position = np.array([self.point_relative_to_plane(point) for point in points])\n",
    "        #position_x = np.array([self.point_relative_to_plane_x(point)-offset for point in points])\n",
    "        #print(position - position_x)\n",
    "        return position\n",
    "\n",
    "def generate_splitting_plane(points):\n",
    "    #feature_ranges = get_feature_ranges(points)\n",
    "    origin = np.fromiter(generate_point(points), dtype=float)\n",
    "    normal = np.fromiter(generate_point(points), dtype=float)\n",
    "    #origin = np.zeros_like(normal)\n",
    "    #normal -= origin\n",
    "    return Hyperplane(origin=origin, normal=normal)\n",
    "    \n",
    "def generate_point(points):\n",
    "    \"\"\" Generat an n-dimensional normal vector\n",
    "\n",
    "    For now just do so by sampling the points from a uniform distribution.\n",
    "    \"\"\"\n",
    "    feature_mins_maxes = get_feature_ranges(points)\n",
    "    for min_, max_, in get_feature_ranges(points):\n",
    "        yield np.random.uniform(low=min_, high=max_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning plane fit...\n",
      "done fitting\n",
      "Average anomalous depth: 2.58\n",
      "Average non-anomalous depth: 3.28736842105\n",
      "\n",
      "Done plane simul-----\n",
      "\n",
      "Beginning iforest fit...\n",
      "done fitting\n",
      "Average anomalous depth: 8.62\n",
      "Average non-anomalous depth: 19.3263157895\n"
     ]
    }
   ],
   "source": [
    "N_ESTIMATORS = 1\n",
    "SCORE_AT = 2.5\n",
    "\n",
    "n = 1000 # number of entries\n",
    "p = 2    # features\n",
    "\n",
    "infection_pct = 0.05\n",
    "X, y = _gen_easy_data(n, p, infection_pct)\n",
    "\n",
    "scores_r, depths_r, y_pred_r, y_r = run_plane_simul(X, y)\n",
    "print(\"\\nDone plane simul-----\\n\")\n",
    "scores_i, depths_i, y_pred_i, y_i = run_iforest_simul(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -1.37280703,  -0.56024005],\n",
       "       [ 11.36570884,   3.77988007],\n",
       "       [  3.34012657,  13.34362564],\n",
       "       [  4.02681731,  11.14744039],\n",
       "       [ 17.42444546,  11.60953577],\n",
       "       [ -9.08602304, -12.92151409],\n",
       "       [ 15.14425805,  12.70707588],\n",
       "       [ -3.39544144,   6.9006884 ],\n",
       "       [  1.07552752,  11.03316679],\n",
       "       [ -6.8240313 ,   9.86519819],\n",
       "       [ 16.71098235,  -8.28630956],\n",
       "       [ -2.11746045,   1.40887617],\n",
       "       [  5.39372271, -16.69124418],\n",
       "       [  2.16933758,  28.97520456],\n",
       "       [-24.94376518,  11.00170154],\n",
       "       [  1.23544578,   4.59481833],\n",
       "       [  3.69013028,  10.72801301],\n",
       "       [ -6.18609373,  -3.05111084],\n",
       "       [  8.22573896,  12.85200319],\n",
       "       [  4.95286347,  -7.01886689],\n",
       "       [ 15.40701759,  13.5103146 ],\n",
       "       [  6.69622409,  22.20154637],\n",
       "       [  0.19989231,   7.20879773],\n",
       "       [ 17.38344867,  -4.9211051 ],\n",
       "       [  5.56288721,   7.45250685],\n",
       "       [  3.53438398,  10.14586437],\n",
       "       [  5.13897434,  -3.63779169],\n",
       "       [ 22.22622343,  13.04001446],\n",
       "       [ 18.44452552,  -8.20876384],\n",
       "       [ 24.99038919,  -6.57169737],\n",
       "       [ 11.38022015,   5.64735688],\n",
       "       [ 17.27175048,  21.16650902],\n",
       "       [  8.87651049,  10.96454788],\n",
       "       [  2.77075613, -11.84951633],\n",
       "       [  9.92807304,   8.96770924],\n",
       "       [ 10.96161242,  16.43033502],\n",
       "       [ 12.49511925,  17.50280529],\n",
       "       [ 22.66471125,  27.91605402],\n",
       "       [  6.14698374,  -3.1292686 ],\n",
       "       [ 10.14131444,  16.98297957],\n",
       "       [ 12.36308278,  -9.24311886],\n",
       "       [  2.48253059,   9.17550229],\n",
       "       [  5.7132626 ,  13.80338341],\n",
       "       [  2.51081266,   9.50450128],\n",
       "       [ -1.45077557,  10.67651462],\n",
       "       [ -1.47594008,   8.09641122],\n",
       "       [-16.50305813,  -8.97655378],\n",
       "       [ 13.76820381,   5.83669158],\n",
       "       [  5.48638312,   1.44368366],\n",
       "       [ -3.23007071,  12.20230652]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[np.where(y==1.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "rhp = RandomHyperplanes(n_estimators=N_ESTIMATORS)\n",
    "rhp = rhp.fit(X)\n",
    "depths = rhp.get_depths(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2.   4.   6.  10.  18.  20.  21.  25.  29.  33.  34.  35.  37.  38.  39.\n",
      "  40.  42.  44.  46.  48.  50.]\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(depths))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
