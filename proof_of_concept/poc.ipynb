{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Proof of concept for the random-cut-hyperplanes idea \"\"\"\n",
    "import sys\n",
    "import numpy as np\n",
    "from scipy.stats import scoreatpercentile\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "N_ESTIMATORS = 100\n",
    "SCORE_AT = 2.5\n",
    "\n",
    "# sys.setrecursionlimit(50000)\n",
    "\n",
    "# Steps for the algorithm\n",
    "#\n",
    "# Given a dataset, X:\n",
    "#   1. Get the feature ranges for each feature within X.\n",
    "#   2. Sample a random point between each feature.\n",
    "#   3. Create a hyperplane using each of these points.\n",
    "#   4. Get the normal to that plane.\n",
    "#   5. Determine which side of the plane the point lies on.\n",
    "#   6. Repeat for the two sides.\n",
    "#   7. If an individual point is left, stop iteration\n",
    "#\n",
    "class IsolationForest(object):\n",
    "    def __init__(self, n_estimators=10):\n",
    "        self.n_estimators = n_estimators\n",
    "\n",
    "    def fit(self, points):\n",
    "        self.trees = []\n",
    "        for i in range(self.n_estimators):\n",
    "            self.trees.append(IsolationTree(points))\n",
    "        return self\n",
    "\n",
    "    def decision_function(self, points):\n",
    "        depths = np.array([tree.decision_function(points) for tree in self.trees])\n",
    "        mean_depths = np.mean(depths, axis=0)\n",
    "        # Normalize the points\n",
    "        scores = 2**-(mean_depths / average_path_length(points.shape[0]))\n",
    "        return scores\n",
    "\n",
    "    def get_depths(self, points):\n",
    "        depths = np.array([tree.decision_function(points) for tree in self.trees])\n",
    "        mean_depths = np.mean(depths, axis=0)\n",
    "        return mean_depths\n",
    "\n",
    "\n",
    "class IsolationTree(object):\n",
    "    def __init__(self, points, group_threshold=15, depth=1):\n",
    "        self.child_left = self.child_right = None\n",
    "        self.points = points\n",
    "        self.group_threshold = group_threshold\n",
    "        self.num_points = self.points.shape[0]\n",
    "        self.split(self.points, depth)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"<{self.__class__.__name__} num_points:{self.num_points}>\"\n",
    "\n",
    "    @property\n",
    "    def is_leaf(self):\n",
    "        return self.child_left == None and self.child_right == None\n",
    "\n",
    "    def split(self, points, depth=1):\n",
    "        node, points_left, points_right = self.get_split(points)\n",
    "\n",
    "        if not node or depth >= 50:\n",
    "            return self\n",
    "\n",
    "        self.node = node\n",
    "        self.child_left = IsolationTree(points_left, depth=depth + 1)\n",
    "        self.child_right = IsolationTree(points_right, depth=depth + 1)\n",
    "\n",
    "    def decision_function(self, points):\n",
    "        return np.array([self.get_depth(point) for point in points])\n",
    "\n",
    "    def get_depth(self, point):\n",
    "        if self.is_leaf:\n",
    "            return 1\n",
    "        else:\n",
    "            if self.node.is_point_left(point):\n",
    "                return 1 + self.child_left.get_depth(point)\n",
    "            else:\n",
    "                return 1 + self.child_right.get_depth(point)\n",
    "\n",
    "    def get_split(self, points):\n",
    "        if self.num_points <2:\n",
    "            return (None, None, None)\n",
    "        split_feature = sample_feature(points)\n",
    "        split_threshold = sample_split_threshold(points, split_feature)\n",
    "        node = Node(split_threshold, split_feature)\n",
    "\n",
    "        positions = node.position_of_points(points)\n",
    "\n",
    "        points_left = points[positions]\n",
    "        points_right = points[np.logical_not(positions)]\n",
    "\n",
    "        return (node, points_left, points_right)\n",
    "\n",
    "\n",
    "class Node(object):\n",
    "    def __init__(self, threshold, feature):\n",
    "        self.threshold = threshold\n",
    "        self.feature = feature\n",
    "\n",
    "    def is_point_left(self, point):\n",
    "        return point[self.feature] < self.threshold\n",
    "\n",
    "    def position_of_points(self, points):\n",
    "        return np.array([self.is_point_left(point) for point in points])\n",
    "\n",
    "\n",
    "class RandomHyperplanes(object):\n",
    "    def __init__(self, n_estimators=10):\n",
    "        self.n_estimators = n_estimators\n",
    "\n",
    "    def fit(self, points):\n",
    "        self.planes = []\n",
    "        for i in range(self.n_estimators): \n",
    "            self.planes.append(HyperplaneCollection(points))\n",
    "        return self\n",
    "\n",
    "    def decision_function(self, points):\n",
    "        depths = np.array([plane.decision_function(points) for plane in self.planes])\n",
    "        mean_depths = np.mean(depths, axis=0)\n",
    "        # Normalize the points\n",
    "        scores = 2**-(mean_depths / average_path_length(points.shape[0]))\n",
    "        return scores\n",
    "\n",
    "    def get_depths(self, points):\n",
    "        depths = np.array([plane.decision_function(points) for plane in self.planes])\n",
    "        mean_depths = np.mean(depths, axis=0)\n",
    "        return mean_depths\n",
    "\n",
    "\n",
    "class HyperplaneCollection(object):\n",
    "    def __init__(self, points, group_threshold=15, depth=1):\n",
    "        self.child_left = self.child_right = None\n",
    "        self.points = points\n",
    "        self.group_threshold = group_threshold\n",
    "        self.num_points = self.points.shape[0]\n",
    "        self.split(self.points, depth)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"<{self.__class__.__name__} num_points:{self.num_points}>\"\n",
    "\n",
    "    @property\n",
    "    def is_leaf(self):\n",
    "        return self.child_left == None and self.child_right == None\n",
    "\n",
    "    def split(self, points, depth=1):\n",
    "        plane, points_left, points_right = self.get_split(points)\n",
    "        if not plane or depth >= 50:\n",
    "            return self\n",
    "        self.splitting_plane = plane\n",
    "        self.child_left = HyperplaneCollection(points_left, depth=depth + 1)\n",
    "        self.child_right = HyperplaneCollection(points_right, depth=depth + 1)\n",
    "\n",
    "    def decision_function(self, points):\n",
    "        return np.array([self.get_depth(point) for point in points])\n",
    "\n",
    "    def get_depth(self, point):\n",
    "        if self.is_leaf:\n",
    "            return 1\n",
    "        else:\n",
    "            if self.splitting_plane.point_relative_to_plane(point) < 0:\n",
    "                return 1 + self.child_left.get_depth(point)\n",
    "            else:\n",
    "                return 1 + self.child_right.get_depth(point)\n",
    "\n",
    "    def get_split(self, points):\n",
    "        if self.num_points < 2:\n",
    "            return (None, None, None)\n",
    "        \n",
    "        splitting_plane = generate_splitting_plane(points)\n",
    "        positions = splitting_plane.position_of_points(points)\n",
    "        split_point = np.random.uniform(np.min(positions), np.max(positions))\n",
    "        \n",
    "        points_left = points[np.where(positions < split_point)]\n",
    "        points_right = points[np.where(positions >= split_point)[0]]\n",
    "        return (splitting_plane, points_left, points_right)\n",
    "\n",
    "\n",
    "class Hyperplane(object):\n",
    "    def __init__(self, origin, normal):\n",
    "        self.origin = origin\n",
    "        self.normal = normal\n",
    "\n",
    "    def point_relative_to_plane(self, point):\n",
    "        return np.dot(self.normal, point - self.origin)\n",
    "\n",
    "    def position_of_points(self, points):\n",
    "        position = np.array([self.point_relative_to_plane(point) for point in points])\n",
    "        return position\n",
    "\n",
    "\n",
    "def sample_feature(point):\n",
    "    length = point.shape[-1]\n",
    "    return np.random.randint(low=0, high=length)\n",
    "\n",
    "def sample_split_threshold(points, feature):\n",
    "    return list(generate_point(points))[feature]\n",
    "\n",
    "def average_path_length(n):\n",
    "    return 2 * harmonic_approx(n - 1) - (2 * (n - 1) / n)\n",
    "\n",
    "def harmonic_approx(n):\n",
    "    return np.log(n) + 0.5772156649\n",
    "\n",
    "def generate_splitting_plane(points):\n",
    "    feature_ranges = get_feature_ranges(points)\n",
    "    origin = np.fromiter(generate_point(points), dtype=float)\n",
    "    \n",
    "    normal = np.fromiter(generate_point(points), dtype=float)\n",
    "    normal -= origin\n",
    "    \n",
    "    return Hyperplane(origin=origin, normal=normal)\n",
    "    \n",
    "def generate_point(points):\n",
    "    \"\"\" Generat an n-dimensional normal vector\n",
    "\n",
    "    For now just do so by sampling the points from a uniform distribution.\n",
    "    \"\"\"\n",
    "    feature_mins_maxes = get_feature_ranges(points)\n",
    "    for min_, max_, in get_feature_ranges(points):\n",
    "        yield np.random.uniform(low=min_, high=max_)\n",
    "\n",
    "def get_feature_ranges(points):\n",
    "    \"\"\" Yields the min and max of each feature in a vector. \"\"\"\n",
    "    points_transpose = np.transpose(points)\n",
    "    for point in points_transpose:\n",
    "        yield (np.min(point), np.max(point))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def _gen_hard_data(n, p, infection_pct, variance=10.0, mu=5.0):\n",
    "    X = np.random.randn(n, p)\n",
    "\n",
    "    # hard data\n",
    "    # Weight it to the number of features\n",
    "    is_anomaly = np.random.rand(n, p) < (infection_pct / p)\n",
    "    X[is_anomaly] = variance * np.random.randn() + mu\n",
    "\n",
    "    y = np.zeros(shape=(n,))\n",
    "\n",
    "    tmp = np.array([np.any(r) for r in is_anomaly])\n",
    "    y[tmp] = 1.0\n",
    "\n",
    "    return (X, y)\n",
    "\n",
    "def _gen_easy_data(n, p, infection_pct, variance=10.0, mu=5.0):\n",
    "    X = np.random.randn(n, p)\n",
    "    is_anomaly = np.random.choice(n, size=int(infection_pct*n), replace=False)\n",
    "    X[is_anomaly] = variance * np.random.randn(is_anomaly.shape[0], p) + mu\n",
    "    y = np.zeros(shape=(n,))\n",
    "    y[is_anomaly] = 1.0\n",
    "    return (X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def run_plane_simul(points, y):\n",
    "    print(\"Beginning plane fit...\")\n",
    "    rhp = RandomHyperplanes(n_estimators=N_ESTIMATORS)\n",
    "    rhp = rhp.fit(points)\n",
    "    print(\"done fitting\")\n",
    "\n",
    "    scores = rhp.decision_function(points)\n",
    "    threshold = scoreatpercentile(scores, 100 - SCORE_AT)\n",
    "    anomalies = scores >= threshold\n",
    "    y_pred = np.zeros(shape=anomalies.shape)\n",
    "    y_pred[anomalies] = 1\n",
    "\n",
    "    correct_guesses = np.count_nonzero(y[np.where(scores <= threshold)])\n",
    "    incorrect_guesses = y[np.where(scores <= threshold)].shape[0] - \\\n",
    "        correct_guesses\n",
    "\n",
    "    print(\"Correct guesses:\", correct_guesses)\n",
    "    print(\"Incorrect guesses:\", incorrect_guesses)\n",
    "    print(\"Expected\", np.count_nonzero(y), \"anomalies\")\n",
    "\n",
    "    cnf_matrix = confusion_matrix(y, y_pred)\n",
    "\n",
    "    tn, fp, fn, tp = cnf_matrix.ravel()\n",
    "    print(f\"tp: {tp} \\ntn: {tn} \\nfp: {fp} \\nfn: {fn}\")\n",
    "\n",
    "    cnf_matrix = cnf_matrix.astype('float') / \\\n",
    "        cnf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    tn, fp, fn, tp = cnf_matrix.ravel()\n",
    "    print(f\"Normalized \\ntp: {tp} \\ntn: {tn} \\nfp: {fp} \\nfn: {fn}\")\n",
    "\n",
    "    print(cnf_matrix)\n",
    "\n",
    "    depths = rhp.get_depths(points)\n",
    "    anomalous_depths = depths[np.where(y==1.0)]\n",
    "    print(\"Average anomalous depth:\", np.mean(anomalous_depths))\n",
    "\n",
    "    non_anomalous_depths = depths[np.where(y==0.0)]\n",
    "    print(\"Average non-anomalous depth:\", np.mean(non_anomalous_depths))\n",
    "    return (scores, depths, y_pred, y)\n",
    "\n",
    "\n",
    "def run_iforest_simul(points, y):\n",
    "    print(\"Beginning iforest fit...\")\n",
    "    iforest = IsolationForest(n_estimators=N_ESTIMATORS)\n",
    "    iforest = iforest.fit(points)\n",
    "    print(\"done fitting\")\n",
    "\n",
    "    scores = iforest.decision_function(points)\n",
    "    threshold = scoreatpercentile(scores, 100 - SCORE_AT)\n",
    "    anomalies = scores >= threshold\n",
    "    y_pred = np.zeros(shape=anomalies.shape)\n",
    "    y_pred[anomalies] = 1\n",
    "\n",
    "    correct_guesses = np.count_nonzero(y[np.where(scores <= threshold)])\n",
    "    incorrect_guesses = y[np.where(scores <= threshold)].shape[0] - \\\n",
    "        correct_guesses\n",
    "\n",
    "    print(\"iforest Correct guesses:\", correct_guesses)\n",
    "    print(\"iforest Incorrect guesses:\", incorrect_guesses)\n",
    "    print(\"Expected\", np.count_nonzero(y), \"anomalies\")\n",
    "\n",
    "    iforest_cnf_matrix = confusion_matrix(y, y_pred)\n",
    "    tn, fp, fn, tp = iforest_cnf_matrix.ravel()\n",
    "    print(f\"tp: {tp} \\ntn: {tn} \\nfp: {fp} \\nfn: {fn}\")\n",
    "\n",
    "    iforest_cnf_matrix = iforest_cnf_matrix.astype('float') / \\\n",
    "            iforest_cnf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    print(iforest_cnf_matrix)\n",
    "\n",
    "    tn, fp, fn, tp = iforest_cnf_matrix.ravel()\n",
    "    print(f\"Normalized \\ntp: {tp} \\ntn: {tn} \\nfp: {fp} \\nfn: {fn}\")\n",
    "\n",
    "    depths = iforest.get_depths(points)\n",
    "    anomalous_depths = depths[np.where(y==1.0)]\n",
    "    print(\"Average anomalous depth:\", np.mean(anomalous_depths))\n",
    "\n",
    "    non_anomalous_depths = depths[np.where(y==0.0)]\n",
    "    print(\"Average non-anomalous depth:\", np.mean(non_anomalous_depths))\n",
    "    return (scores, depths, y_pred, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning plane fit...\n",
      "done fitting\n",
      "Correct guesses: 25\n",
      "Incorrect guesses: 950\n",
      "Expected 50 anomalies\n",
      "tp: 25 \n",
      "tn: 950 \n",
      "fp: 0 \n",
      "fn: 25\n",
      "Normalized \n",
      "tp: 0.5 \n",
      "tn: 1.0 \n",
      "fp: 0.0 \n",
      "fn: 0.5\n",
      "[[ 1.   0. ]\n",
      " [ 0.5  0.5]]\n",
      "Average anomalous depth: 4.9154\n",
      "Average non-anomalous depth: 5.39808421053\n",
      "\n",
      "Done plane simul-----\n",
      "\n",
      "Beginning iforest fit...\n",
      "done fitting\n",
      "iforest Correct guesses: 25\n",
      "iforest Incorrect guesses: 950\n",
      "Expected 50 anomalies\n",
      "tp: 25 \n",
      "tn: 950 \n",
      "fp: 0 \n",
      "fn: 25\n",
      "[[ 1.   0. ]\n",
      " [ 0.5  0.5]]\n",
      "Normalized \n",
      "tp: 0.5 \n",
      "tn: 1.0 \n",
      "fp: 0.0 \n",
      "fn: 0.5\n",
      "Average anomalous depth: 9.142\n",
      "Average non-anomalous depth: 21.3118210526\n"
     ]
    }
   ],
   "source": [
    "n = 1000 # number of entries\n",
    "p = 2      # features\n",
    "\n",
    "infection_pct = 0.05\n",
    "X, y = _gen_easy_data(n, p, infection_pct)\n",
    "\n",
    "scores_r, depths_r, y_pred_r, y_r = run_plane_simul(X, y)\n",
    "print(\"\\nDone plane simul-----\\n\")\n",
    "scores_i, depths_i, y_pred_i, y_i = run_iforest_simul(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 18.60054873, -10.95517404],\n",
       "       [ -6.62049433,   8.659407  ],\n",
       "       [  6.73332824,   3.2713922 ],\n",
       "       [  3.94500525, -18.22435418],\n",
       "       [  0.11572348,  17.35247173],\n",
       "       [ 16.60699083,  -3.31381237],\n",
       "       [ 16.20571523,   9.96942915],\n",
       "       [ 13.68927114,  22.24338862],\n",
       "       [ -3.98345534, -10.10624978],\n",
       "       [  0.92679046,   5.25535703],\n",
       "       [ -9.15699072,   3.09623447],\n",
       "       [  1.98245599,   4.92405339],\n",
       "       [  7.37499162,  -2.95429831],\n",
       "       [ 26.99341035,  30.02256101],\n",
       "       [ 12.42164119,  -2.19454332],\n",
       "       [  4.85163682,  17.64654949],\n",
       "       [ 13.2260233 ,  13.76336832],\n",
       "       [  7.54969002,  20.45818724],\n",
       "       [ -5.60013908,   0.16208662],\n",
       "       [ 10.78554622,  -0.97770086],\n",
       "       [-11.87904608,   3.67210198],\n",
       "       [-12.49403585,   8.40398242],\n",
       "       [  8.54301419,  12.34880688],\n",
       "       [ 12.13679903,  25.39733537],\n",
       "       [ -2.58267732,   0.92450868],\n",
       "       [ -1.60236641,  11.79389096],\n",
       "       [  5.24908799,  -4.03209561],\n",
       "       [  7.06201363,   8.05355695],\n",
       "       [ 15.99469035,  -8.17883705],\n",
       "       [ -0.21914551,  25.42265371],\n",
       "       [ -7.87761813,  19.2412379 ],\n",
       "       [  8.62762679,  11.09089816],\n",
       "       [ 16.46392646, -10.00580437],\n",
       "       [  9.52966331,   4.48943972],\n",
       "       [  9.50964219,  14.62415996],\n",
       "       [  7.39189789,   9.8096804 ],\n",
       "       [ 23.48028515,  -0.26893475],\n",
       "       [  6.05435104,  12.42576244],\n",
       "       [ 11.50137538,   7.614581  ],\n",
       "       [  1.65030864,   7.28539031],\n",
       "       [ 16.72157243,  10.58543878],\n",
       "       [ 18.10640296,  10.89377401],\n",
       "       [ -5.96351387,  20.68144319],\n",
       "       [ -1.90253997,  21.34911558],\n",
       "       [  7.49620402,  22.40827261],\n",
       "       [-15.15876661,  16.45651413],\n",
       "       [  6.64366915,   7.53839717],\n",
       "       [ -3.36670669,  -0.11903104],\n",
       "       [  1.15295371,  14.3300565 ],\n",
       "       [ 12.11861662,   6.0358121 ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[np.where(y==1.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
