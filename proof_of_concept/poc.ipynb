{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Proof of concept for the random-cut-hyperplanes idea \"\"\"\n",
    "import sys\n",
    "import numpy as np\n",
    "from scipy.stats import scoreatpercentile\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "N_ESTIMATORS = 100\n",
    "SCORE_AT = 2.5\n",
    "\n",
    "# sys.setrecursionlimit(50000)\n",
    "\n",
    "# Steps for the algorithm\n",
    "#\n",
    "# Given a dataset, X:\n",
    "#   1. Get the feature ranges for each feature within X.\n",
    "#   2. Sample a random point between each feature.\n",
    "#   3. Create a hyperplane using each of these points.\n",
    "#   4. Get the normal to that plane.\n",
    "#   5. Determine which side of the plane the point lies on.\n",
    "#   6. Repeat for the two sides.\n",
    "#   7. If an individual point is left, stop iteration\n",
    "#\n",
    "class IsolationForest(object):\n",
    "    def __init__(self, n_estimators=10):\n",
    "        self.n_estimators = n_estimators\n",
    "\n",
    "    def fit(self, points):\n",
    "        self.trees = []\n",
    "        for i in range(self.n_estimators):\n",
    "            self.trees.append(IsolationTree(points))\n",
    "        return self\n",
    "\n",
    "    def decision_function(self, points):\n",
    "        depths = np.array([tree.decision_function(points) for tree in self.trees])\n",
    "        mean_depths = np.mean(depths, axis=0)\n",
    "        # Normalize the points\n",
    "        scores = 2**-(mean_depths / average_path_length(points.shape[0]))\n",
    "        return scores\n",
    "\n",
    "    def get_depths(self, points):\n",
    "        depths = np.array([tree.decision_function(points) for tree in self.trees])\n",
    "        mean_depths = np.mean(depths, axis=0)\n",
    "        return mean_depths\n",
    "\n",
    "\n",
    "class IsolationTree(object):\n",
    "    def __init__(self, points, group_threshold=15, depth=1):\n",
    "        self.child_left = self.child_right = None\n",
    "        self.points = points\n",
    "        self.group_threshold = group_threshold\n",
    "        self.num_points = self.points.shape[0]\n",
    "        self.split(self.points, depth)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"<{self.__class__.__name__} num_points:{self.num_points}>\"\n",
    "\n",
    "    @property\n",
    "    def is_leaf(self):\n",
    "        return self.child_left == None and self.child_right == None\n",
    "\n",
    "    def split(self, points, depth=1):\n",
    "        node, points_left, points_right = self.get_split(points)\n",
    "\n",
    "        if not node or depth >= 50:\n",
    "            return self\n",
    "\n",
    "        self.node = node\n",
    "        self.child_left = IsolationTree(points_left, depth=depth + 1)\n",
    "        self.child_right = IsolationTree(points_right, depth=depth + 1)\n",
    "\n",
    "    def decision_function(self, points):\n",
    "        return np.array([self.get_depth(point) for point in points])\n",
    "\n",
    "    def get_depth(self, point):\n",
    "        if self.is_leaf:\n",
    "            return 1\n",
    "        else:\n",
    "            if self.node.is_point_left(point):\n",
    "                return 1 + self.child_left.get_depth(point)\n",
    "            else:\n",
    "                return 1 + self.child_right.get_depth(point)\n",
    "\n",
    "    def get_split(self, points):\n",
    "        if points.shape[0] <= self.group_threshold:\n",
    "            return (None, None, None)\n",
    "\n",
    "        split_feature = sample_feature(points)\n",
    "        split_threshold = sample_split_threshold(points, split_feature)\n",
    "        node = Node(split_threshold, split_feature)\n",
    "\n",
    "        positions = node.position_of_points(points)\n",
    "\n",
    "        points_left = points[positions]\n",
    "        points_right = points[np.logical_not(positions)]\n",
    "\n",
    "        return (node, points_left, points_right)\n",
    "\n",
    "\n",
    "class Node(object):\n",
    "    def __init__(self, threshold, feature):\n",
    "        self.threshold = threshold\n",
    "        self.feature = feature\n",
    "\n",
    "    def is_point_left(self, point):\n",
    "        return point[self.feature] < self.threshold\n",
    "\n",
    "    def position_of_points(self, points):\n",
    "        return np.array([self.is_point_left(point) for point in points])\n",
    "\n",
    "\n",
    "class RandomHyperplanes(object):\n",
    "    def __init__(self, n_estimators=10):\n",
    "        self.n_estimators = n_estimators\n",
    "\n",
    "    def fit(self, points):\n",
    "        self.planes = list(self._fit(points))\n",
    "        return self\n",
    "\n",
    "    def _fit(self, points):\n",
    "        for i in range(self.n_estimators):\n",
    "            yield HyperplaneCollection(points)\n",
    "\n",
    "    def decision_function(self, points):\n",
    "        depths = np.array([plane.decision_function(points) for plane in self.planes])\n",
    "        mean_depths = np.mean(depths, axis=0)\n",
    "        # Normalize the points\n",
    "        scores = 2**-(mean_depths / average_path_length(points.shape[0]))\n",
    "        return scores\n",
    "\n",
    "    def get_depths(self, points):\n",
    "        depths = np.array([plane.decision_function(points) for plane in self.planes])\n",
    "        mean_depths = np.mean(depths, axis=0)\n",
    "        return mean_depths\n",
    "\n",
    "\n",
    "class HyperplaneCollection(object):\n",
    "    def __init__(self, points, group_threshold=15, depth=1):\n",
    "        self.child_left = self.child_right = None\n",
    "        self.points = points\n",
    "        self.group_threshold = group_threshold\n",
    "        self.num_points = self.points.shape[0]\n",
    "        self.split(self.points, depth)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"<{self.__class__.__name__} num_points:{self.num_points}>\"\n",
    "\n",
    "    @property\n",
    "    def is_leaf(self):\n",
    "        return self.child_left == None and self.child_right == None\n",
    "\n",
    "    def split(self, points, depth=1):\n",
    "        plane, points_left, points_right = self.get_split(points)\n",
    "\n",
    "        if not plane or depth >= 50:\n",
    "            return self\n",
    "\n",
    "        self.splitting_plane = plane\n",
    "        self.child_left = HyperplaneCollection(points_left, depth=depth + 1)\n",
    "        self.child_right = HyperplaneCollection(points_right, depth=depth + 1)\n",
    "\n",
    "    def decision_function(self, points):\n",
    "        return np.array([self.get_depth(point) for point in points])\n",
    "\n",
    "    def get_depth(self, point):\n",
    "        if self.is_leaf:\n",
    "            return 1\n",
    "        else:\n",
    "            if self.splitting_plane.point_relative_to_plane(point) < 0:\n",
    "                return 1 + self.child_left.get_depth(point)\n",
    "            else:\n",
    "                return 1 + self.child_right.get_depth(point)\n",
    "\n",
    "    def get_split(self, points):\n",
    "        if points.shape[0] <= self.group_threshold:\n",
    "            return (None, None, None)\n",
    "\n",
    "        splitting_plane = generate_splitting_plane(points)\n",
    "        positions = splitting_plane.position_of_points(points)\n",
    "\n",
    "        points_left = points[np.where(positions < 0)]\n",
    "        points_right = points[np.where(positions > 0)]\n",
    "\n",
    "        return (splitting_plane, points_left, points_right)\n",
    "\n",
    "\n",
    "class Hyperplane(object):\n",
    "    def __init__(self, origin, normal):\n",
    "        self.origin = origin\n",
    "        self.normal = normal\n",
    "\n",
    "    def point_relative_to_plane(self, point):\n",
    "        return np.dot(self.normal, point - self.origin)\n",
    "\n",
    "    def position_of_points(self, points):\n",
    "        position = np.array([self.point_relative_to_plane(point) for point in points])\n",
    "        return position\n",
    "\n",
    "\n",
    "def sample_feature(point):\n",
    "    length = point.shape[-1]\n",
    "    return np.random.randint(low=0, high=length)\n",
    "\n",
    "def sample_split_threshold(points, feature):\n",
    "    return list(generate_point(points))[feature]\n",
    "\n",
    "def average_path_length(n):\n",
    "    return 2 * harmonic_approx(n - 1) - (2 * (n - 1) / n)\n",
    "\n",
    "def harmonic_approx(n):\n",
    "    return np.log(n) + 0.5772156649\n",
    "\n",
    "def generate_splitting_plane(points):\n",
    "    # Generate n points for each feature range\n",
    "    # Use those as the coefs of a normal\n",
    "    # Split on those points\n",
    "\n",
    "    feature_ranges = get_feature_ranges(points)\n",
    "    origin = np.zeros(shape=points.shape[-1])\n",
    "\n",
    "    normal = np.fromiter(generate_point(points), dtype=float)\n",
    "    normal -= origin\n",
    "\n",
    "    return Hyperplane(origin=origin, normal=normal)\n",
    "\n",
    "\n",
    "def generate_point(points):\n",
    "    \"\"\" Generat an n-dimensional normal vector\n",
    "\n",
    "    For now just do so by sampling the points from a uniform distribution.\n",
    "    \"\"\"\n",
    "    feature_mins_maxes = get_feature_ranges(points)\n",
    "    for min_, max_, in get_feature_ranges(points):\n",
    "        yield np.random.uniform(low=min_, high=max_)\n",
    "\n",
    "def get_feature_ranges(points):\n",
    "    \"\"\" Yields the min and max of each feature in a vector. \"\"\"\n",
    "    points_transpose = np.transpose(points)\n",
    "    for point in points_transpose:\n",
    "        yield (np.min(point), np.max(point))\n",
    "\n",
    "def randomly_shuffle_points(points):\n",
    "    points_prime = points\n",
    "    np.random.shuffle(points_prime)\n",
    "    points_prime = points_prime[0:points_prime.shape[-1]] # Make it square\n",
    "    return points_prime\n",
    "\n",
    "def calculate_normal(points, origin):\n",
    "    points_inv = np.linalg.inv(points)\n",
    "    solution_vec = np.ones(points_inv.shape[0])\n",
    "    # while np.all(solution_vec > 0) or \\\n",
    "        #         np.all(solution_vec < 0) or \\\n",
    "        #         np.any(solution_vec == 0):\n",
    "    solution_vec = np.random.uniform(\n",
    "        low=-1.0, high=1.0, size=(points_inv.shape[0], ))\n",
    "\n",
    "    normal = np.matmul(points_inv, solution_vec)\n",
    "    return normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def _gen_hard_data(n, p, infection_pct, variance=10.0, mu=5.0):\n",
    "    X = np.random.randn(n, p)\n",
    "\n",
    "    # hard data\n",
    "    # Weight it to the number of features\n",
    "    is_anomaly = np.random.rand(n, p) < (infection_pct / p)\n",
    "    X[is_anomaly] = variance * np.random.randn() + mu\n",
    "\n",
    "    y = np.zeros(shape=(n,))\n",
    "\n",
    "    tmp = np.array([np.any(r) for r in is_anomaly])\n",
    "    y[tmp] = 1.0\n",
    "\n",
    "    return (X, y)\n",
    "\n",
    "def _gen_easy_data(n, p, infection_pct, variance=10.0, mu=5.0):\n",
    "    X = np.random.randn(n, p)\n",
    "    is_anomaly = np.random.choice(n, size=int(infection_pct*n), replace=False)\n",
    "    X[is_anomaly] = variance * np.random.randn(is_anomaly.shape[0], p) + mu\n",
    "    y = np.zeros(shape=(n,))\n",
    "    y[is_anomaly] = 1.0\n",
    "    return (X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def run_plane_simul(points, y):\n",
    "    print(\"Beginning plane fit...\")\n",
    "    rhp = RandomHyperplanes(n_estimators=N_ESTIMATORS)\n",
    "    rhp = rhp.fit(points)\n",
    "    print(\"done fitting\")\n",
    "\n",
    "    scores = rhp.decision_function(points)\n",
    "    threshold = scoreatpercentile(scores, 100 - SCORE_AT)\n",
    "    anomalies = scores >= threshold\n",
    "    y_pred = np.zeros(shape=anomalies.shape)\n",
    "    y_pred[anomalies] = 1\n",
    "\n",
    "    correct_guesses = np.count_nonzero(y[np.where(scores <= threshold)])\n",
    "    incorrect_guesses = y[np.where(scores <= threshold)].shape[0] - \\\n",
    "        correct_guesses\n",
    "\n",
    "    print(\"Correct guesses:\", correct_guesses)\n",
    "    print(\"Incorrect guesses:\", incorrect_guesses)\n",
    "    print(\"Expected\", np.count_nonzero(y), \"anomalies\")\n",
    "\n",
    "    cnf_matrix = confusion_matrix(y, y_pred)\n",
    "\n",
    "    tn, fp, fn, tp = cnf_matrix.ravel()\n",
    "    print(f\"tp: {tp} \\ntn: {tn} \\nfp: {fp} \\nfn: {fn}\")\n",
    "\n",
    "    cnf_matrix = cnf_matrix.astype('float') / \\\n",
    "        cnf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    tn, fp, fn, tp = cnf_matrix.ravel()\n",
    "    print(f\"Normalized \\ntp: {tp} \\ntn: {tn} \\nfp: {fp} \\nfn: {fn}\")\n",
    "\n",
    "    print(cnf_matrix)\n",
    "\n",
    "    depths = rhp.get_depths(points)\n",
    "    anomalous_depths = depths[np.where(y==1.0)]\n",
    "    print(\"Average anomalous depth:\", np.mean(anomalous_depths))\n",
    "\n",
    "    non_anomalous_depths = depths[np.where(y==0.0)]\n",
    "    print(\"Average non-anomalous depth:\", np.mean(non_anomalous_depths))\n",
    "    return (scores, depths, y_pred, y)\n",
    "\n",
    "\n",
    "def run_iforest_simul(points, y):\n",
    "    print(\"Beginning iforest fit...\")\n",
    "    iforest = IsolationForest(n_estimators=N_ESTIMATORS)\n",
    "    iforest = iforest.fit(points)\n",
    "    print(\"done fitting\")\n",
    "\n",
    "    scores = iforest.decision_function(points)\n",
    "    threshold = scoreatpercentile(scores, 100 - SCORE_AT)\n",
    "    anomalies = scores >= threshold\n",
    "    y_pred = np.zeros(shape=anomalies.shape)\n",
    "    y_pred[anomalies] = 1\n",
    "\n",
    "    correct_guesses = np.count_nonzero(y[np.where(scores <= threshold)])\n",
    "    incorrect_guesses = y[np.where(scores <= threshold)].shape[0] - \\\n",
    "        correct_guesses\n",
    "\n",
    "    print(\"iforest Correct guesses:\", correct_guesses)\n",
    "    print(\"iforest Incorrect guesses:\", incorrect_guesses)\n",
    "    print(\"Expected\", np.count_nonzero(y), \"anomalies\")\n",
    "\n",
    "    iforest_cnf_matrix = confusion_matrix(y, y_pred)\n",
    "    tn, fp, fn, tp = iforest_cnf_matrix.ravel()\n",
    "    print(f\"tp: {tp} \\ntn: {tn} \\nfp: {fp} \\nfn: {fn}\")\n",
    "\n",
    "    iforest_cnf_matrix = iforest_cnf_matrix.astype('float') / \\\n",
    "            iforest_cnf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    print(iforest_cnf_matrix)\n",
    "\n",
    "    tn, fp, fn, tp = iforest_cnf_matrix.ravel()\n",
    "    print(f\"Normalized \\ntp: {tp} \\ntn: {tn} \\nfp: {fp} \\nfn: {fn}\")\n",
    "\n",
    "    depths = iforest.get_depths(points)\n",
    "    anomalous_depths = depths[np.where(y==1.0)]\n",
    "    print(\"Average anomalous depth:\", np.mean(anomalous_depths))\n",
    "\n",
    "    non_anomalous_depths = depths[np.where(y==0.0)]\n",
    "    print(\"Average non-anomalous depth:\", np.mean(non_anomalous_depths))\n",
    "    return (scores, depths, y_pred, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning plane fit...\n",
      "done fitting\n",
      "Correct guesses: 50\n",
      "Incorrect guesses: 925\n",
      "Expected 50 anomalies\n",
      "tp: 0 \n",
      "tn: 925 \n",
      "fp: 25 \n",
      "fn: 50\n",
      "Normalized \n",
      "tp: 0.0 \n",
      "tn: 0.9736842105263158 \n",
      "fp: 0.02631578947368421 \n",
      "fn: 1.0\n",
      "[[ 0.97368421  0.02631579]\n",
      " [ 1.          0.        ]]\n",
      "Average anomalous depth: 7.9476\n",
      "Average non-anomalous depth: 11.4587897436\n",
      "\n",
      "Done plane simul-----\n",
      "\n",
      "Beginning iforest fit...\n",
      "done fitting\n",
      "iforest Correct guesses: 25\n",
      "iforest Incorrect guesses: 950\n",
      "Expected 50 anomalies\n",
      "tp: 25 \n",
      "tn: 950 \n",
      "fp: 0 \n",
      "fn: 25\n",
      "[[ 1.   0. ]\n",
      " [ 0.5  0.5]]\n",
      "Normalized \n",
      "tp: 0.5 \n",
      "tn: 1.0 \n",
      "fp: 0.0 \n",
      "fn: 0.5\n",
      "Average anomalous depth: 6.004\n",
      "Average non-anomalous depth: 22.2711692308\n"
     ]
    }
   ],
   "source": [
    "n = 1000 # number of entries\n",
    "p = 50  # features\n",
    "\n",
    "infection_pct = 0.05\n",
    "X, y = _gen_easy_data(n, p, infection_pct)\n",
    "\n",
    "scores_r, depths_r, y_pred_r, y_r = run_plane_simul(X, y)\n",
    "print(\"\\nDone plane simul-----\\n\")\n",
    "scores_i, depths_i, y_pred_i, y_i = run_iforest_simul(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X[np.where(y==1.0)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
