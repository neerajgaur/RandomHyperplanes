{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Proof of concept for the random-cut-hyperplanes idea \"\"\"\n",
    "import sys\n",
    "import numpy as np\n",
    "from scipy.stats import scoreatpercentile\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "N_ESTIMATORS = 100\n",
    "SCORE_AT = 2.5\n",
    "\n",
    "# sys.setrecursionlimit(50000)\n",
    "\n",
    "# Steps for the algorithm\n",
    "#\n",
    "# Given a dataset, X:\n",
    "#   1. Get the feature ranges for each feature within X.\n",
    "#   2. Sample a random point between each feature.\n",
    "#   3. Create a hyperplane using each of these points.\n",
    "#   4. Get the normal to that plane.\n",
    "#   5. Determine which side of the plane the point lies on.\n",
    "#   6. Repeat for the two sides.\n",
    "#   7. If an individual point is left, stop iteration\n",
    "#\n",
    "class IsolationForest(object):\n",
    "    def __init__(self, n_estimators=10):\n",
    "        self.n_estimators = n_estimators\n",
    "\n",
    "    def fit(self, points):\n",
    "        self.trees = []\n",
    "        for i in range(self.n_estimators):\n",
    "            self.trees.append(IsolationTree(points))\n",
    "        return self\n",
    "\n",
    "    def decision_function(self, points):\n",
    "        depths = np.array([tree.decision_function(points) for tree in self.trees])\n",
    "        mean_depths = np.mean(depths, axis=0)\n",
    "        # Normalize the points\n",
    "        scores = 2**-(mean_depths / average_path_length(points.shape[0]))\n",
    "        return scores\n",
    "\n",
    "    def get_depths(self, points):\n",
    "        depths = np.array([tree.decision_function(points) for tree in self.trees])\n",
    "        mean_depths = np.mean(depths, axis=0)\n",
    "        return mean_depths\n",
    "\n",
    "\n",
    "class IsolationTree(object):\n",
    "    def __init__(self, points, group_threshold=15, depth=1):\n",
    "        self.child_left = self.child_right = None\n",
    "        self.points = points\n",
    "        self.group_threshold = group_threshold\n",
    "        self.num_points = self.points.shape[0]\n",
    "        self.split(self.points, depth)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"<{self.__class__.__name__} num_points:{self.num_points}>\"\n",
    "\n",
    "    @property\n",
    "    def is_leaf(self):\n",
    "        return self.child_left == None and self.child_right == None\n",
    "\n",
    "    def split(self, points, depth=1):\n",
    "        node, points_left, points_right = self.get_split(points)\n",
    "\n",
    "        if not node or depth >= 50:\n",
    "            return self\n",
    "\n",
    "        self.node = node\n",
    "        self.child_left = IsolationTree(points_left, depth=depth + 1)\n",
    "        self.child_right = IsolationTree(points_right, depth=depth + 1)\n",
    "\n",
    "    def decision_function(self, points):\n",
    "        return np.array([self.get_depth(point) for point in points])\n",
    "\n",
    "    def get_depth(self, point):\n",
    "        if self.is_leaf:\n",
    "            return 1\n",
    "        else:\n",
    "            if self.node.is_point_left(point):\n",
    "                return 1 + self.child_left.get_depth(point)\n",
    "            else:\n",
    "                return 1 + self.child_right.get_depth(point)\n",
    "\n",
    "    def get_split(self, points):\n",
    "        if self.num_points <2:\n",
    "            return (None, None, None)\n",
    "        split_feature = sample_feature(points)\n",
    "        split_threshold = sample_split_threshold(points, split_feature)\n",
    "        node = Node(split_threshold, split_feature)\n",
    "\n",
    "        positions = node.position_of_points(points)\n",
    "\n",
    "        points_left = points[positions]\n",
    "        points_right = points[np.logical_not(positions)]\n",
    "\n",
    "        return (node, points_left, points_right)\n",
    "\n",
    "\n",
    "class Node(object):\n",
    "    def __init__(self, threshold, feature):\n",
    "        self.threshold = threshold\n",
    "        self.feature = feature\n",
    "\n",
    "    def is_point_left(self, point):\n",
    "        return point[self.feature] < self.threshold\n",
    "\n",
    "    def position_of_points(self, points):\n",
    "        return np.array([self.is_point_left(point) for point in points])\n",
    "\n",
    "\n",
    "class RandomHyperplanes(object):\n",
    "    def __init__(self, n_estimators=10):\n",
    "        self.n_estimators = n_estimators\n",
    "\n",
    "    def fit(self, points):\n",
    "        self.planes = []\n",
    "        for i in range(self.n_estimators): \n",
    "            self.planes.append(HyperplaneCollection(points))\n",
    "        return self\n",
    "\n",
    "    def decision_function(self, points):\n",
    "        depths = np.array([plane.decision_function(points) for plane in self.planes])\n",
    "        mean_depths = np.mean(depths, axis=0)\n",
    "        # Normalize the points\n",
    "        scores = 2**-(mean_depths / average_path_length(points.shape[0]))\n",
    "        return scores\n",
    "\n",
    "    def get_depths(self, points):\n",
    "        depths = np.array([plane.decision_function(points) for plane in self.planes])\n",
    "        mean_depths = np.mean(depths, axis=0)\n",
    "        return mean_depths\n",
    "\n",
    "\n",
    "class HyperplaneCollection(object):\n",
    "    def __init__(self, points, group_threshold=15, depth=1):\n",
    "        self.child_left = self.child_right = None\n",
    "        self.points = points\n",
    "        self.group_threshold = group_threshold\n",
    "        self.num_points = self.points.shape[0]\n",
    "        self.split(self.points, depth)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"<{self.__class__.__name__} num_points:{self.num_points}>\"\n",
    "\n",
    "    @property\n",
    "    def is_leaf(self):\n",
    "        return self.child_left == None and self.child_right == None\n",
    "\n",
    "    def split(self, points, depth=1):\n",
    "        plane, points_left, points_right = self.get_split(points)\n",
    "        if not plane or depth >= 50:\n",
    "            return self\n",
    "        self.splitting_plane = plane\n",
    "        self.child_left = HyperplaneCollection(points_left, depth=depth + 1)\n",
    "        self.child_right = HyperplaneCollection(points_right, depth=depth + 1)\n",
    "\n",
    "    def decision_function(self, points):\n",
    "        return np.array([self.get_depth(point) for point in points])\n",
    "\n",
    "    def get_depth(self, point):\n",
    "        if self.is_leaf:\n",
    "            return 1\n",
    "        else:\n",
    "            if self.splitting_plane.point_relative_to_plane(point) < 0:\n",
    "                return 1 + self.child_left.get_depth(point)\n",
    "            else:\n",
    "                return 1 + self.child_right.get_depth(point)\n",
    "\n",
    "    def get_split(self, points):\n",
    "        if self.num_points < 2:\n",
    "            return (None, None, None)\n",
    "        splitting_plane = generate_splitting_plane(points)\n",
    "        positions = splitting_plane.position_of_points(points)\n",
    "        split_point = np.random.uniform(np.min(positions), np.max(positions))\n",
    "        points_left = points[np.where(positions < split_point)[0], :]\n",
    "        points_right = points[np.where(positions >= split_point)[0], :]\n",
    "        return (splitting_plane, points_left, points_right)\n",
    "\n",
    "\n",
    "class Hyperplane(object):\n",
    "    def __init__(self, line):\n",
    "        self.line = line\n",
    "\n",
    "    def point_relative_to_plane(self, point):\n",
    "        return np.dot(self.line, point)\n",
    "\n",
    "    def position_of_points(self, points):\n",
    "        position = np.array([self.point_relative_to_plane(point) for point in points])\n",
    "        return position\n",
    "\n",
    "\n",
    "def sample_feature(point):\n",
    "    length = point.shape[-1]\n",
    "    return np.random.randint(low=0, high=length)\n",
    "\n",
    "def sample_split_threshold(points, feature):\n",
    "    return list(generate_point(points))[feature]\n",
    "\n",
    "def average_path_length(n):\n",
    "    return 2 * harmonic_approx(n - 1) - (2 * (n - 1) / n)\n",
    "\n",
    "def harmonic_approx(n):\n",
    "    return np.log(n) + 0.5772156649\n",
    "\n",
    "def generate_splitting_plane(points):\n",
    "    p = points.shape[1]\n",
    "    line = np.random.randn(1, p)\n",
    "    line /= np.linalg.norm(line)\n",
    "    return Hyperplane(line=line)\n",
    "\n",
    "def generate_point(points):\n",
    "    \"\"\" Generat an n-dimensional normal vector\n",
    "\n",
    "    For now just do so by sampling the points from a uniform distribution.\n",
    "    \"\"\"\n",
    "    feature_mins_maxes = get_feature_ranges(points)\n",
    "    for min_, max_, in get_feature_ranges(points):\n",
    "        yield np.random.uniform(low=min_, high=max_)\n",
    "\n",
    "def get_feature_ranges(points):\n",
    "    \"\"\" Yields the min and max of each feature in a vector. \"\"\"\n",
    "    points_transpose = np.transpose(points)\n",
    "    for point in points_transpose:\n",
    "        yield (np.min(point), np.max(point))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def _gen_hard_data(n, p, infection_pct, variance=10.0, mu=5.0):\n",
    "    X = np.random.randn(n, p)\n",
    "\n",
    "    # hard data\n",
    "    # Weight it to the number of features\n",
    "    is_anomaly = np.random.rand(n, p) < (infection_pct / p)\n",
    "    X[is_anomaly] = variance * np.random.randn() + mu\n",
    "\n",
    "    y = np.zeros(shape=(n,))\n",
    "\n",
    "    tmp = np.array([np.any(r) for r in is_anomaly])\n",
    "    y[tmp] = 1.0\n",
    "\n",
    "    return (X, y)\n",
    "\n",
    "def _gen_easy_data(n, p, infection_pct, variance=10.0, mu=5.0):\n",
    "    X = np.random.randn(n, p)\n",
    "    is_anomaly = np.random.choice(n, size=int(infection_pct*n), replace=False)\n",
    "    X[is_anomaly] = variance * np.random.randn(is_anomaly.shape[0], p) + mu\n",
    "    y = np.zeros(shape=(n,))\n",
    "    y[is_anomaly] = 1.0\n",
    "    return (X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def run_plane_simul(points, y):\n",
    "    print(\"Beginning plane fit...\")\n",
    "    rhp = RandomHyperplanes(n_estimators=N_ESTIMATORS)\n",
    "    rhp = rhp.fit(points)\n",
    "    print(\"done fitting\")\n",
    "\n",
    "    scores = rhp.decision_function(points)\n",
    "    threshold = scoreatpercentile(scores, 100 - SCORE_AT)\n",
    "    anomalies = scores >= threshold\n",
    "    y_pred = np.zeros(shape=anomalies.shape)\n",
    "    y_pred[anomalies] = 1\n",
    "\n",
    "    correct_guesses = np.count_nonzero(y[np.where(scores <= threshold)])\n",
    "    incorrect_guesses = y[np.where(scores <= threshold)].shape[0] - \\\n",
    "        correct_guesses\n",
    "\n",
    "    print(\"Correct guesses:\", correct_guesses)\n",
    "    print(\"Incorrect guesses:\", incorrect_guesses)\n",
    "    print(\"Expected\", np.count_nonzero(y), \"anomalies\")\n",
    "\n",
    "    cnf_matrix = confusion_matrix(y, y_pred)\n",
    "\n",
    "    tn, fp, fn, tp = cnf_matrix.ravel()\n",
    "    print(f\"tp: {tp} \\ntn: {tn} \\nfp: {fp} \\nfn: {fn}\")\n",
    "\n",
    "    cnf_matrix = cnf_matrix.astype('float') / \\\n",
    "        cnf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    tn, fp, fn, tp = cnf_matrix.ravel()\n",
    "    print(f\"Normalized \\ntp: {tp} \\ntn: {tn} \\nfp: {fp} \\nfn: {fn}\")\n",
    "\n",
    "    print(cnf_matrix)\n",
    "\n",
    "    depths = rhp.get_depths(points)\n",
    "    anomalous_depths = depths[np.where(y==1.0)]\n",
    "    print(\"Average anomalous depth:\", np.mean(anomalous_depths))\n",
    "\n",
    "    non_anomalous_depths = depths[np.where(y==0.0)]\n",
    "    print(\"Average non-anomalous depth:\", np.mean(non_anomalous_depths))\n",
    "    return (scores, depths, y_pred, y)\n",
    "\n",
    "\n",
    "def run_iforest_simul(points, y):\n",
    "    print(\"Beginning iforest fit...\")\n",
    "    iforest = IsolationForest(n_estimators=N_ESTIMATORS)\n",
    "    iforest = iforest.fit(points)\n",
    "    print(\"done fitting\")\n",
    "\n",
    "    scores = iforest.decision_function(points)\n",
    "    threshold = scoreatpercentile(scores, 100 - SCORE_AT)\n",
    "    anomalies = scores >= threshold\n",
    "    y_pred = np.zeros(shape=anomalies.shape)\n",
    "    y_pred[anomalies] = 1\n",
    "\n",
    "    correct_guesses = np.count_nonzero(y[np.where(scores <= threshold)])\n",
    "    incorrect_guesses = y[np.where(scores <= threshold)].shape[0] - \\\n",
    "        correct_guesses\n",
    "\n",
    "    print(\"iforest Correct guesses:\", correct_guesses)\n",
    "    print(\"iforest Incorrect guesses:\", incorrect_guesses)\n",
    "    print(\"Expected\", np.count_nonzero(y), \"anomalies\")\n",
    "\n",
    "    iforest_cnf_matrix = confusion_matrix(y, y_pred)\n",
    "    tn, fp, fn, tp = iforest_cnf_matrix.ravel()\n",
    "    print(f\"tp: {tp} \\ntn: {tn} \\nfp: {fp} \\nfn: {fn}\")\n",
    "\n",
    "    iforest_cnf_matrix = iforest_cnf_matrix.astype('float') / \\\n",
    "            iforest_cnf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    print(iforest_cnf_matrix)\n",
    "\n",
    "    tn, fp, fn, tp = iforest_cnf_matrix.ravel()\n",
    "    print(f\"Normalized \\ntp: {tp} \\ntn: {tn} \\nfp: {fp} \\nfn: {fn}\")\n",
    "\n",
    "    depths = iforest.get_depths(points)\n",
    "    anomalous_depths = depths[np.where(y==1.0)]\n",
    "    print(\"Average anomalous depth:\", np.mean(anomalous_depths))\n",
    "\n",
    "    non_anomalous_depths = depths[np.where(y==0.0)]\n",
    "    print(\"Average non-anomalous depth:\", np.mean(non_anomalous_depths))\n",
    "    return (scores, depths, y_pred, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning plane fit...\n",
      "done fitting\n",
      "Correct guesses: 50\n",
      "Incorrect guesses: 926\n",
      "Expected 50 anomalies\n",
      "tp: 0 \n",
      "tn: 924 \n",
      "fp: 26 \n",
      "fn: 50\n",
      "Normalized \n",
      "tp: 0.0 \n",
      "tn: 0.9726315789473684 \n",
      "fp: 0.02736842105263158 \n",
      "fn: 1.0\n",
      "[[ 0.97263158  0.02736842]\n",
      " [ 1.          0.        ]]\n",
      "Average anomalous depth: 5.0146\n",
      "Average non-anomalous depth: 4.94476842105\n",
      "\n",
      "Done plane simul-----\n",
      "\n",
      "Beginning iforest fit...\n",
      "done fitting\n",
      "iforest Correct guesses: 25\n",
      "iforest Incorrect guesses: 950\n",
      "Expected 50 anomalies\n",
      "tp: 25 \n",
      "tn: 950 \n",
      "fp: 0 \n",
      "fn: 25\n",
      "[[ 1.   0. ]\n",
      " [ 0.5  0.5]]\n",
      "Normalized \n",
      "tp: 0.5 \n",
      "tn: 1.0 \n",
      "fp: 0.0 \n",
      "fn: 0.5\n",
      "Average anomalous depth: 9.2074\n",
      "Average non-anomalous depth: 26.0247052632\n"
     ]
    }
   ],
   "source": [
    "n = 1000 # number of entries\n",
    "p = 2  # features\n",
    "\n",
    "infection_pct = 0.05\n",
    "X, y = _gen_easy_data(n, p, infection_pct)\n",
    "\n",
    "scores_r, depths_r, y_pred_r, y_r = run_plane_simul(X, y)\n",
    "print(\"\\nDone plane simul-----\\n\")\n",
    "scores_i, depths_i, y_pred_i, y_i = run_iforest_simul(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.3205387 ,  -6.21877095],\n",
       "       [ -8.66868595,  -2.38940586],\n",
       "       [  1.07424663,  29.0168159 ],\n",
       "       [  4.44467867,   7.82640735],\n",
       "       [  8.52945019,   7.64287503],\n",
       "       [  7.10311549,   1.21573248],\n",
       "       [ -6.05574913,  25.3638925 ],\n",
       "       [  4.54879918,  -1.49609521],\n",
       "       [ -6.21247271,   9.98821282],\n",
       "       [ 13.62065119, -11.29898294],\n",
       "       [ 16.08962047,   2.8399161 ],\n",
       "       [ 34.5212479 ,  -0.24847133],\n",
       "       [ -2.78427368,  12.57325845],\n",
       "       [ 12.63118791,  -1.63127773],\n",
       "       [ -7.91594554,   2.99853338],\n",
       "       [  4.05696303,   0.03602262],\n",
       "       [ -6.35508734,  15.79217985],\n",
       "       [ -7.72859004,  15.48437573],\n",
       "       [  2.96254381,  28.30703324],\n",
       "       [-14.23969413,   3.71893053],\n",
       "       [ -0.2793386 ,   1.51305066],\n",
       "       [ 12.30806124,  -9.21924361],\n",
       "       [ 12.5059899 ,   8.60175258],\n",
       "       [-11.82170378,   4.00283936],\n",
       "       [ -0.84421552,   2.47599458],\n",
       "       [ -1.10982818,   6.26469928],\n",
       "       [  1.4937637 ,   2.58156363],\n",
       "       [-18.47410149,   4.54197565],\n",
       "       [ -5.33946271,  10.74898283],\n",
       "       [  6.18646643,   1.67296586],\n",
       "       [ 13.82073995,  13.4006275 ],\n",
       "       [-11.05035821,   5.17343428],\n",
       "       [  9.8560556 ,  -2.43271282],\n",
       "       [ 16.60283933,   1.92727657],\n",
       "       [ 14.63788635,   4.84972252],\n",
       "       [ 13.49426579,   3.74777205],\n",
       "       [ 23.45990623,  -1.23321533],\n",
       "       [  7.30972409, -19.56078633],\n",
       "       [ -0.39087127,  -8.06568032],\n",
       "       [ 10.14105145,  21.81353549],\n",
       "       [  8.68784277,   2.18304971],\n",
       "       [ 11.49179145,   7.65225509],\n",
       "       [  3.32805919,   5.90711672],\n",
       "       [ 16.7348937 ,   5.45674757],\n",
       "       [ 15.29538221,  -8.1721621 ],\n",
       "       [ -5.84601065,   7.03880667],\n",
       "       [ -4.47661033,   6.65982573],\n",
       "       [  3.85950805,  -7.05650993],\n",
       "       [ 25.82920125,   0.0955559 ],\n",
       "       [  2.16054362,  18.45005185]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[np.where(y==1.0)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
